{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Разные подходы к определению языка\n",
    "\n",
    "В этой работе, я опробую дла способа обработки (подготовки) текста для написания алгоритма по распознованию языка.\n",
    "Сперва мы мы опробуем метод токенезации текста, а затем построим n-gramm'ы.\n",
    "\n",
    "Импортируем необходимы нем модули."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import collections\n",
    "import wikipedia as wiki\n",
    "import warnings\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from itertools import islice, tee\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выберем 14 языков.\n",
    "\n",
    "P.S. \n",
    "\n",
    "Сперва их было 15, но с греческим вознили проблемы при скачивании текстов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "langs = ['ru', 'be', 'sl', 'mn', 'pl',\n",
    "       'ja', 'it', 'cs', 'hr',\n",
    "       'fr', 'uk', 'tr', 'tt', 'sk']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию, которая будет скачивать статьи из википедии для указанного языка."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_texts_for_lang(lang, n=10):\n",
    "    wiki.set_lang(lang)\n",
    "    wiki_content = []\n",
    "    pages = wiki.random(n)\n",
    "    for page_name in pages:\n",
    "        try:\n",
    "            page = wiki.page(page_name)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        wiki_content.append('{}\\n{}'.format(page.title, page.content.replace('==', '')))\n",
    "\n",
    "    return wiki_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wiki_texts = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачиваем тексты и помещаем их в созданный выше словарь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cs 94\n",
      "hr 99\n",
      "fr 93\n",
      "uk 93\n",
      "tr 97\n",
      "tt 96\n",
      "sk 93\n"
     ]
    }
   ],
   "source": [
    "for lang in langs2:\n",
    "    wiki_texts[lang] = get_texts_for_lang(lang, 100)\n",
    "    print(lang, len(wiki_texts[lang]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сперва попробуем метод токенизации\n",
    "\n",
    "Напишем свою фенкцию токенизации текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    text = re.sub(r'[^\\w\\s]','',text).replace('\\n', '')\n",
    "    text = re.sub(r'\\d','',text)\n",
    "    text = re.sub(r'[\\s]{2,}',' ',text)\n",
    "    return text.split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь необходимо составить частотный словарь для каждлого языка, а затем проверим получившийся сллварь для какого-нибудь языка. Для примера – Японский."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tr\t16539\t7266\n",
      "ru\t40710\t15245\n",
      "sl\t18284\t7844\n",
      "uk\t24559\t10804\n",
      "it\t46408\t11444\n",
      "sk\t12275\t5489\n",
      "tt\t9391\t3466\n",
      "pl\t19350\t8578\n",
      "fr\t41554\t10251\n",
      "hr\t21016\t8580\n",
      "be\t15875\t7181\n",
      "mn\t20034\t7973\n",
      "ja\t5128\t3906\n",
      "cs\t35301\t13381\n",
      "0.009945397815912636\t外部リンク\n",
      "0.0093603744149766\t脚注\n",
      "0.008775351014040561\tisbn\n",
      "0.0074102964118564745\tthe\n",
      "0.007215288611544462\t年月日\n"
     ]
    }
   ],
   "source": [
    "result_dict = collections.defaultdict(lambda: collections.defaultdict(lambda: 0))\n",
    "\n",
    "for lang in langs:\n",
    "    for article in wiki_texts[lang]:\n",
    "        for word in tokenize(article.replace('\\n', '').lower()):\n",
    "            result_dict[lang][word] += 1\n",
    "\n",
    "for lang in wiki_texts.keys():\n",
    "    print('{}\\t{}\\t{}'.format(lang, sum(result_dict[lang].values()), len(result_dict[lang])))\n",
    "\n",
    "            \n",
    "for lang in wiki_texts.keys():\n",
    "    summ = sum(result_dict[lang].values())\n",
    "    for word in result_dict[lang]:\n",
    "        result_dict[lang][word] = result_dict[lang][word]/summ\n",
    "\n",
    "for lang in wiki_texts.keys():\n",
    "    result_dict[lang] = {i[0]:i[1] for i in collections.Counter(result_dict[lang]).most_common(500)}\n",
    "    \n",
    "\n",
    "for word in sorted(result_dict['ja'], key=lambda w: result_dict['ja'][w], reverse=True)[:5]:\n",
    "    print('{}\\t{}'.format(result_dict['ja'][word], word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отлично, дальше напишем функцию которая будет предсказывать язык, по получившейся дате. И сразу же попробуем определить язык, для какого-нибудь простого языка."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_dict_prediction(text, data):\n",
    "    words = {}\n",
    "    for lang in data.keys():\n",
    "        count = 0\n",
    "        for word in tokenize(text):\n",
    "            if word in data[lang]:\n",
    "                count += data[lang][word]\n",
    "        words[lang] = count\n",
    "    \n",
    "    result = max(words, key=lambda x: words[x]) \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'it'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_dict_prediction('io non parlo italiano', result_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Неплохо, теперь составим репорт по классификации, с помощью эстиматора, который мы написали."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         be       0.97      0.99      0.98        96\n",
      "         cs       0.92      0.72      0.81        94\n",
      "         fr       0.85      1.00      0.92        93\n",
      "         hr       0.89      0.97      0.93        99\n",
      "         it       0.99      0.99      0.99       100\n",
      "         ja       0.90      0.88      0.89        92\n",
      "         mn       1.00      0.97      0.98       100\n",
      "         pl       1.00      0.98      0.99        92\n",
      "         ru       0.80      0.98      0.88        91\n",
      "         sk       0.82      0.81      0.81        93\n",
      "         sl       0.83      0.96      0.89        94\n",
      "         tr       0.93      0.87      0.90        97\n",
      "         tt       1.00      0.95      0.97        96\n",
      "         uk       0.99      0.74      0.85        93\n",
      "\n",
      "avg / total       0.92      0.92      0.91      1330\n",
      "\n",
      "[[95  0  0  0  0  1  0  0  0  0  0  0  0  0]\n",
      " [ 0 68  1  3  0  0  0  0  0 13  7  2  0  0]\n",
      " [ 0  0 93  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  1 96  0  2  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1 99  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  1  2  1  1 81  0  0  0  0  3  3  0  0]\n",
      " [ 0  1  0  0  0  0 97  0  0  1  0  1  0  0]\n",
      " [ 0  0  2  0  0  0  0 90  0  0  0  0  0  0]\n",
      " [ 0  0  1  0  0  0  0  0 89  0  0  0  0  1]\n",
      " [ 0  2  0  7  0  0  0  0  0 75  9  0  0  0]\n",
      " [ 0  0  0  0  0  1  0  0  0  3 90  0  0  0]\n",
      " [ 0  1  9  0  0  3  0  0  0  0  0 84  0  0]\n",
      " [ 0  0  1  0  0  2  0  0  2  0  0  0 91  0]\n",
      " [ 3  1  0  0  0  0  0  0 20  0  0  0  0 69]]\n"
     ]
    }
   ],
   "source": [
    "target = []\n",
    "prediction = []\n",
    "\n",
    "for lang in wiki_texts.keys():\n",
    "    for text in wiki_texts[lang]:\n",
    "        target.append(lang)\n",
    "        prediction.append(make_dict_prediction(text, result_dict))\n",
    "\n",
    "print(classification_report(target, prediction))\n",
    "print(confusion_matrix(target, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Теперь попробуем метод n-граммов\n",
    "\n",
    "И сразу же напишем функцию для создания такого n-gramma. (N=3 в нашем случае)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_to_ngramm(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]','',text).replace('\\n', '')\n",
    "    text = re.sub(r'\\d','',text)\n",
    "    text = re.sub(r'[\\s]{2,}',' ',text)\n",
    "    N = 3 \n",
    "    output = zip(*(islice(seq, index, None) for index, seq in enumerate(tee(text, N))))\n",
    "    output = [''.join(x) for x in output]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь создаем словарь с использованием написанной нами функцией."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tr\t126837\t6941\n",
      "ru\t296910\t12558\n",
      "sl\t126135\t6588\n",
      "uk\t179046\t11325\n",
      "it\t304163\t6273\n",
      "sk\t85769\t7666\n",
      "tt\t69994\t7791\n",
      "pl\t142722\t8185\n",
      "fr\t261079\t7224\n",
      "hr\t143824\t6530\n",
      "be\t115961\t9518\n",
      "mn\t143779\t10609\n",
      "ja\t218955\t112782\n",
      "cs\t237512\t11115\n",
      "0.0030645566440592816\tている\n",
      "0.0028316320705167727\t年月日\n",
      "0.002160261240894248\tていた\n",
      "0.001991276746363408\tしてい\n",
      "0.0018588294398392364\tとして\n"
     ]
    }
   ],
   "source": [
    "result_ngramm = collections.defaultdict(lambda: collections.defaultdict(lambda: 0))\n",
    "\n",
    "for lang in langs:\n",
    "    for article in wiki_texts[lang]:\n",
    "        for word in convert_to_ngramm(article.replace('\\n', '').lower()):\n",
    "            result_ngramm[lang][word] += 1\n",
    "\n",
    "for lang in wiki_texts.keys():\n",
    "    print('{}\\t{}\\t{}'.format(lang, sum(result_ngramm[lang].values()), len(result_ngramm[lang])))\n",
    "\n",
    "            \n",
    "for lang in wiki_texts.keys():\n",
    "    summ = sum(result_ngramm[lang].values())\n",
    "    for word in result_ngramm[lang]:\n",
    "        result_ngramm[lang][word] = result_ngramm[lang][word]/summ\n",
    "\n",
    "for lang in wiki_texts.keys():\n",
    "    result_ngramm[lang] = {i[0]:i[1] for i in collections.Counter(result_ngramm[lang]).most_common(500)}\n",
    "    \n",
    "\n",
    "for word in sorted(result_ngramm['ja'], key=lambda w: result_ngramm['ja'][w], reverse=True)[:5]:\n",
    "    print('{}\\t{}'.format(result_ngramm['ja'][word], word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пишем новую функцию для определения языка, уже по новым данным"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_ngramm_prediction(text, data):\n",
    "    words = {}\n",
    "    for lang in data.keys():\n",
    "        count = 0\n",
    "        for word in tokenize(text):\n",
    "            if word in data[lang]:\n",
    "                count += data[lang][word]\n",
    "        words[lang] = count\n",
    "    \n",
    "    result = max(words, key=lambda x: words[x]) \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И наконец проверяем, какой же результат мы получаем..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         be       0.67      0.59      0.63        96\n",
      "         cs       0.91      0.82      0.86        94\n",
      "         fr       0.79      1.00      0.88        93\n",
      "         hr       0.83      0.56      0.67        99\n",
      "         it       0.71      0.91      0.80       100\n",
      "         ja       1.00      0.16      0.28        92\n",
      "         mn       0.85      0.73      0.78       100\n",
      "         pl       0.85      0.65      0.74        92\n",
      "         ru       0.74      0.68      0.71        91\n",
      "         sk       0.90      0.47      0.62        93\n",
      "         sl       0.75      0.56      0.64        94\n",
      "         tr       0.27      0.96      0.42        97\n",
      "         tt       0.68      0.42      0.52        96\n",
      "         uk       0.78      0.56      0.65        93\n",
      "\n",
      "avg / total       0.76      0.65      0.66      1330\n",
      "\n",
      "[[57  0  1  0  2  0  0  0  7  0  0 17  6  6]\n",
      " [ 0 77  2  2  1  0  0  1  0  1  1  9  0  0]\n",
      " [ 0  0 93  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  4 55  6  0  0  3  0  1  7 23  0  0]\n",
      " [ 0  0  0  1 91  0  0  0  0  0  0  8  0  0]\n",
      " [ 0  0  4  0 10 15  5  0  0  0  0 58  0  0]\n",
      " [ 1  0  2  0  1  0 73  0  2  0  0 11 10  0]\n",
      " [ 0  7  1  0  2  0  0 60  0  3  1 18  0  0]\n",
      " [ 1  0  1  0  1  0  2  0 62  0  0 12  3  9]\n",
      " [ 0  1  0  3  0  0  0  5  0 44  8 32  0  0]\n",
      " [ 0  0  3  5 12  0  0  2  0  0 53 19  0  0]\n",
      " [ 0  0  4  0  0  0  0  0  0  0  0 93  0  0]\n",
      " [25  0  1  0  1  0  4  0  0  0  1 24 40  0]\n",
      " [ 1  0  2  0  1  0  2  0 13  0  0 22  0 52]]\n"
     ]
    }
   ],
   "source": [
    "target = []\n",
    "prediction = []\n",
    "\n",
    "for lang in wiki_texts.keys():\n",
    "    for text in wiki_texts[lang]:\n",
    "        target.append(lang)\n",
    "        prediction.append(make_ngramm_prediction(text, result_ngramm))\n",
    "\n",
    "print(classification_report(target, prediction))\n",
    "print(confusion_matrix(target, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подеведем итоги...\n",
    "\n",
    "Для нашей выборки текстов, лучше себя показал алгоритм с использованием токенизации.\n",
    "Скорее всего это связано с тем, что во многих текстах, которые мы скачали для набора языков находится много вхождений английских слов (английского написания), это можно често увидеть на примере языка 'ja' – японский и 'tr' – турецкий.\n",
    "\n",
    "Скорее всего, при дополнительной обработке текстов или другим наборе языков, точность метода N-gramm изменилась бы."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
